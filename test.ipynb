{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from pandas.core.common import flatten\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # Define the layers of the network\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=32, kernel_size=3, padding=1, stride=1\n",
    "        )\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1\n",
    "        )\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1\n",
    "        )\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1\n",
    "        )\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc4 = nn.Linear(in_features=256 * 16 * 16, out_features=512)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.fc5 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the layers of the network\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet-based CNN model\n",
    "class ResNetCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNetCNN, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained ResNet-18 model\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        \n",
    "        # Remove the last fully connected layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "        # Add a new fully connected layer\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the ResNet model\n",
    "        x = self.resnet(x)\n",
    "        \n",
    "        # Flatten the output tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass the output through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 0 images in Dataset\n",
      "There are 2 directories and 0 images in Dataset\\test\n",
      "There are 0 directories and 100 images in Dataset\\test\\Defected\n",
      "There are 0 directories and 100 images in Dataset\\test\\Undefected\n",
      "There are 2 directories and 0 images in Dataset\\train\n",
      "There are 0 directories and 400 images in Dataset\\train\\Defected\n",
      "There are 0 directories and 400 images in Dataset\\train\\Undefected\n",
      "['Defected' 'Undefected']\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(\"Dataset\"):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in {dirpath}\")\n",
    "\n",
    "bean_class = len(os.listdir(\"Dataset/train\"))\n",
    "\n",
    "dataset_dir = pathlib.Path(\"Dataset/train\")\n",
    "class_names = np.array(sorted([item.name for item in dataset_dir.glob(\"*\")]))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_defect_images = glob.glob(\"Dataset/train/Defected/*.jpg\")\n",
    "# train_undefect_images = glob.glob(\"Dataset/train/Undefected/*.jpg\")\n",
    "\n",
    "# train_concat = train_defect_images + train_undefect_images\n",
    "\n",
    "# train_image_paths = list(flatten(train_concat))\n",
    "# random.shuffle(train_image_paths)\n",
    "\n",
    "# train_image_paths, test_image_paths = train_image_paths[:int(0.8*len(train_image_paths))], train_image_paths[:int(0.2*len(train_image_paths))]\n",
    "# print(len(train_image_paths), len(test_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters and optimizer\n",
    "num_epochs = 8\n",
    "learning_rate = 0.001\n",
    "batch_size = 8\n",
    "num_classes = 2\n",
    "\n",
    "# Own Model\n",
    "model = MyModel(num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Step 2: Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229,0.224,0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the dataset\n",
    "train_set = datasets.ImageFolder(root='Dataset/train/', transform=transform)\n",
    "\n",
    "# Define a data loader to iterate over the dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the dataset\n",
    "test_set = datasets.ImageFolder(root='Dataset/test/', transform=transform)\n",
    "\n",
    "# Define a data loader to iterate over the dataset\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LnData User\\Documents\\TA GASS\\Defect-Classification\\.venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Batch   100] loss: 0.568\n",
      "Training Accuracy: 88.25%\n",
      "[Epoch 2, Batch   100] loss: 0.269\n",
      "Training Accuracy: 95.12%\n",
      "[Epoch 3, Batch   100] loss: 0.174\n",
      "Training Accuracy: 96.88%\n",
      "[Epoch 4, Batch   100] loss: 0.273\n",
      "Training Accuracy: 97.12%\n",
      "[Epoch 5, Batch   100] loss: 0.112\n",
      "Training Accuracy: 98.50%\n",
      "[Epoch 6, Batch   100] loss: 0.113\n",
      "Training Accuracy: 98.00%\n",
      "[Epoch 7, Batch   100] loss: 0.057\n",
      "Training Accuracy: 94.88%\n",
      "[Epoch 8, Batch   100] loss: 0.051\n",
      "Training Accuracy: 99.88%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
    "                  (epoch+1, i+1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc = correct / total\n",
    "    print('Training Accuracy: {:.2f}%'.format(train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        predictions.extend(predicted.numpy())\n",
    "        ground_truths.extend(labels.numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = correct / total\n",
    "print('Validating Accuracy: {:.2f}%'.format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(ground_truths, predictions)\n",
    "labels_category = ['Defected', 'Undefected']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels= labels_category, yticklabels=labels_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LnData User\\Documents\\TA GASS\\Defect-Classification\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LnData User\\Documents\\TA GASS\\Defect-Classification\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to C:\\Users\\LnData User/.cache\\torch\\hub\\checkpoints\\resnet152-394f9c45.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "model = ResNetCNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LnData User\\Documents\\TA GASS\\Defect-Classification\\.venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 89.12%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     29\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     32\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LnData User\\Documents\\TA GASS\\Defect-Classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m, in \u001b[0;36mResNetCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     17\u001b[0m     \u001b[39m# Pass the input through the ResNet model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresnet(x)\n\u001b[0;32m     20\u001b[0m     \u001b[39m# Flatten the output tensor\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LnData User\\Documents\\TA GASS\\Defect-Classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\LnData User\\Documents\\TA GASS\\Defect-Classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\LnData User\\Documents\\TA GASS\\Defect-Classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\LnData User\\Documents\\TA GASS\\Defect-Classification\\.venv\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\LnData User\\Documents\\TA GASS\\Defect-Classification\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2452\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print('[Epoch %d, Batch %5d] Loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 200))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc = correct / total\n",
    "    print('Training Accuracy: {:.2f}%'.format(train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        predictions.extend(predicted.numpy())\n",
    "        ground_truths.extend(labels.numpy())\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = correct / total\n",
    "print('Testing Accuracy: {:.2f}%'.format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(ground_truths, predictions)\n",
    "labels_category = ['Defected', 'Undefected']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels= labels_category, yticklabels=labels_category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
